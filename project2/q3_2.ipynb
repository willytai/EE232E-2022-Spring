{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsJ2MMnxdX6e",
    "outputId": "b2e01a3b-740c-47a2-e275-0119b727b4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEkyoGomdvLT",
    "outputId": "4cda9002-2a5c-4428-b627-83cbc234f48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/ECE 232E\n",
      "--2022-04-28 02:04:38--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: ‘cora.tgz.4’\n",
      "\n",
      "cora.tgz.4          100%[===================>] 164.11K   414KB/s    in 0.4s    \n",
      "\n",
      "2022-04-28 02:04:39 (414 KB/s) - ‘cora.tgz.4’ saved [168052/168052]\n",
      "\n",
      "cora/\n",
      "cora/README\n",
      "cora/cora.cites\n",
      "cora/cora.content\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/ECE\\ 232E\n",
    "!wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "!tar -zxvf cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8W-reoYBea7-"
   },
   "outputs": [],
   "source": [
    "# node2vec from https://github.com/aditya-grover/node2vec.git\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "class Graph():\n",
    "\tdef __init__(self, nx_G, is_directed, p, q):\n",
    "\t\tself.G = nx_G\n",
    "\t\tself.is_directed = is_directed\n",
    "\t\tself.p = p\n",
    "\t\tself.q = q\n",
    "\n",
    "\tdef node2vec_walk(self, walk_length, start_node):\n",
    "\t\t'''\n",
    "\t\tSimulate a random walk starting from start node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\talias_nodes = self.alias_nodes\n",
    "\t\talias_edges = self.alias_edges\n",
    "\n",
    "\t\twalk = [start_node]\n",
    "\n",
    "\t\twhile len(walk) < walk_length:\n",
    "\t\t\tcur = walk[-1]\n",
    "\t\t\tcur_nbrs = sorted(G.neighbors(cur))\n",
    "\t\t\tif len(cur_nbrs) > 0:\n",
    "\t\t\t\tif len(walk) == 1:\n",
    "\t\t\t\t\twalk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprev = walk[-2]\n",
    "\t\t\t\t\tnext = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
    "\t\t\t\t\t\talias_edges[(prev, cur)][1])]\n",
    "\t\t\t\t\twalk.append(next)\n",
    "\t\t\telse:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn walk\n",
    "\n",
    "\tdef simulate_walks(self, num_walks, walk_length):\n",
    "\t\t'''\n",
    "\t\tRepeatedly simulate random walks from each node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\twalks = []\n",
    "\t\tnodes = list(G.nodes())\n",
    "\t\tprint ('Walk iteration:')\n",
    "\t\tfor walk_iter in range(num_walks):\n",
    "\t\t\tprint (str(walk_iter+1), '/', str(num_walks))\n",
    "\t\t\trandom.shuffle(nodes)\n",
    "\t\t\tfor node in nodes:\n",
    "\t\t\t\twalks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "\t\treturn walks\n",
    "\n",
    "\tdef get_alias_edge(self, src, dst):\n",
    "\t\t'''\n",
    "\t\tGet the alias edge setup lists for a given edge.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\tp = self.p\n",
    "\t\tq = self.q\n",
    "\n",
    "\t\tunnormalized_probs = []\n",
    "\t\tfor dst_nbr in sorted(G.neighbors(dst)):\n",
    "\t\t\tif dst_nbr == src:\n",
    "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "\t\t\telif G.has_edge(dst_nbr, src):\n",
    "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tunnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "\t\tnorm_const = sum(unnormalized_probs)\n",
    "\t\tnormalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "\t\treturn alias_setup(normalized_probs)\n",
    "\n",
    "\tdef preprocess_transition_probs(self):\n",
    "\t\t'''\n",
    "\t\tPreprocessing of transition probabilities for guiding the random walks.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\tis_directed = self.is_directed\n",
    "\n",
    "\t\talias_nodes = {}\n",
    "\t\tfor node in G.nodes():\n",
    "\t\t\tunnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "\t\t\tnorm_const = sum(unnormalized_probs)\n",
    "\t\t\tnormalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\t\t\talias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "\t\talias_edges = {}\n",
    "\t\ttriads = {}\n",
    "\n",
    "\t\tif is_directed:\n",
    "\t\t\tfor edge in G.edges():\n",
    "\t\t\t\talias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "\t\telse:\n",
    "\t\t\tfor edge in G.edges():\n",
    "\t\t\t\talias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "\t\t\t\talias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "\t\tself.alias_nodes = alias_nodes\n",
    "\t\tself.alias_edges = alias_edges\n",
    "\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "def alias_setup(probs):\n",
    "\t'''\n",
    "\tCompute utility lists for non-uniform sampling from discrete distributions.\n",
    "\tRefer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "\tfor details\n",
    "\t'''\n",
    "\tK = len(probs)\n",
    "\tq = np.zeros(K)\n",
    "\tJ = np.zeros(K, dtype=np.int)\n",
    "\n",
    "\tsmaller = []\n",
    "\tlarger = []\n",
    "\tfor kk, prob in enumerate(probs):\n",
    "\t    q[kk] = K*prob\n",
    "\t    if q[kk] < 1.0:\n",
    "\t        smaller.append(kk)\n",
    "\t    else:\n",
    "\t        larger.append(kk)\n",
    "\n",
    "\twhile len(smaller) > 0 and len(larger) > 0:\n",
    "\t    small = smaller.pop()\n",
    "\t    large = larger.pop()\n",
    "\n",
    "\t    J[small] = large\n",
    "\t    q[large] = q[large] + q[small] - 1.0\n",
    "\t    if q[large] < 1.0:\n",
    "\t        smaller.append(large)\n",
    "\t    else:\n",
    "\t        larger.append(large)\n",
    "\n",
    "\treturn J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "\t'''\n",
    "\tDraw sample from a non-uniform discrete distribution using alias sampling.\n",
    "\t'''\n",
    "\tK = len(J)\n",
    "\n",
    "\tkk = int(np.floor(np.random.rand()*K))\n",
    "\tif np.random.rand() < q[kk]:\n",
    "\t    return kk\n",
    "\telse:\n",
    "\t    return J[kk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C3ljaoxhj0oJ"
   },
   "outputs": [],
   "source": [
    "# main function https://github.com/aditya-grover/node2vec.git\n",
    "'''\n",
    "Reference implementation of node2vec. \n",
    "\n",
    "Author: Aditya Grover\n",
    "\n",
    "For more details, refer to the paper:\n",
    "node2vec: Scalable Feature Learning for Networks\n",
    "Aditya Grover and Jure Leskovec \n",
    "Knowledge Discovery and Data Mining (KDD), 2016\n",
    "'''\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def read_graph():\n",
    "\t'''\n",
    "\tReads the input network in networkx.\n",
    "\t'''\n",
    "\tG = nx.read_edgelist('./cora/cora.cites', nodetype=int, create_using=nx.DiGraph())\n",
    "\tfor edge in G.edges():\n",
    "\t\tG[edge[0]][edge[1]]['weight'] = 1\n",
    "\tG = G.to_undirected()\n",
    "\treturn G\n",
    "\n",
    "def learn_embeddings(walks, featureSize):\n",
    "\t'''\n",
    "\tLearn embeddings by optimizing the Skipgram objective using SGD.\n",
    "\t'''\n",
    "\twalks = [list(map(str, walk)) for walk in walks]\n",
    "\tmodel = Word2Vec(walks, size=featureSize, window=10, min_count=0, sg=1, workers=8, iter=1)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ASc_X-7kmNa7"
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, featureSize):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense1 = nn.Linear(featureSize, 16)\n",
    "    self.dense2 = nn.Linear(16, 7)\n",
    "    self.activation = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.dense1(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.dense2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PNgpd5R3uYq5"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "class  CoraDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, train_x, train_y):\n",
    "    self.train_x = train_x\n",
    "    self.train_y = train_y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.train_x.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.train_x[idx], self.train_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hVh_8UgeuZKx"
   },
   "outputs": [],
   "source": [
    "def train(trainloader, featureSize, epochs):\n",
    "  from torchsummary import summary\n",
    "  model = Model(featureSize=featureSize)\n",
    "  model.cuda()\n",
    "  summary(model, (featureSize,))\n",
    "  optimizer = torch.optim.Adam(model.parameters())\n",
    "  lossfn = torch.nn.CrossEntropyLoss()\n",
    "  for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "      inputs, labels = data\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "      loss = lossfn(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      running_loss += loss.item()\n",
    "      if i % 100 == 99:\n",
    "          print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "          running_loss = 0.0\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TRZ-Ogf3u_tZ"
   },
   "outputs": [],
   "source": [
    "def test(trainloader, testloader, model, numTrain, numTest):\n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "      for data in trainloader:\n",
    "          inputs, labels = data\n",
    "          # calculate outputs by running images through the network\n",
    "          outputs = model(inputs)\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print(f'Accuracy of the network on the {numTrain} train data: {100 * correct // total} %')\n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          inputs, labels = data\n",
    "          # calculate outputs by running images through the network\n",
    "          outputs = model(inputs)\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print(f'Accuracy of the network on the {numTest} test data: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DkiDPK_To35a"
   },
   "outputs": [],
   "source": [
    "def node2vecmode(p, q, featureSize):\n",
    "  nx_G = read_graph()\n",
    "  G = Graph(nx_G, False, p=p, q=q)\n",
    "  G.preprocess_transition_probs()\n",
    "  walks = G.simulate_walks(10, 80)\n",
    "  model = learn_embeddings(walks, featureSize)\n",
    "\n",
    "  # prepare training data\n",
    "  # read training nodes\n",
    "  import torch\n",
    "  import torch.nn.functional as F\n",
    "  import numpy as np\n",
    "  label2id = {\n",
    "      'Case_Based' : 0,\n",
    "      'Genetic_Algorithms' : 1,\n",
    "      'Neural_Networks' : 2,\n",
    "      'Probabilistic_Methods' : 3,\n",
    "      'Reinforcement_Learning' : 4,\n",
    "      'Rule_Learning' : 5,\n",
    "      'Theory' : 6,\n",
    "  }\n",
    "  data_x = np.zeros((2708, featureSize))\n",
    "  data_y = np.zeros(2708)\n",
    "  count = 0\n",
    "  with open('./cora/cora.content', 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.strip().split()\n",
    "      data_x[count] = model.wv[line[0]]\n",
    "      data_y[count] = label2id[line[-1]]\n",
    "      count += 1\n",
    "  data_x = torch.from_numpy(data_x).to(torch.float32)\n",
    "  data_y = torch.from_numpy(data_y).to(torch.int64)\n",
    "  print(data_x.shape)\n",
    "  print(data_y.shape)\n",
    "\n",
    "  # 10% for validation\n",
    "  shuffle = np.random.permutation(data_x.shape[0])\n",
    "  trainid = shuffle[:int(shuffle.shape[0]*0.9)]\n",
    "  testid = shuffle[int(shuffle.shape[0]*0.9):]\n",
    "\n",
    "  test_x = data_x[testid].cuda()\n",
    "  test_y = data_y[testid].cuda()\n",
    "  train_x = data_x[trainid].cuda()\n",
    "  train_y = data_y[trainid].cuda()\n",
    "\n",
    "  # loader\n",
    "  trainset = CoraDataset(train_x, train_y)\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, 8, shuffle=True)\n",
    "  testset = CoraDataset(test_x, test_y)\n",
    "  testloader = torch.utils.data.DataLoader(testset, 8, shuffle=False)\n",
    "\n",
    "  model = train(trainloader, featureSize, 40)\n",
    "  test(trainloader, testloader, model, trainid.shape[0], testid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZcfZdYQ3-Lq7"
   },
   "outputs": [],
   "source": [
    "def textfeaturemode():\n",
    "  featureSize = 1433\n",
    "  import torch\n",
    "  import torch.nn.functional as F\n",
    "  import numpy as np\n",
    "  label2id = {\n",
    "      'Case_Based' : 0,\n",
    "      'Genetic_Algorithms' : 1,\n",
    "      'Neural_Networks' : 2,\n",
    "      'Probabilistic_Methods' : 3,\n",
    "      'Reinforcement_Learning' : 4,\n",
    "      'Rule_Learning' : 5,\n",
    "      'Theory' : 6,\n",
    "  }\n",
    "  data_x = np.zeros((2708, featureSize))\n",
    "  data_y = np.zeros(2708)\n",
    "  count = 0\n",
    "  with open('./cora/cora.content', 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.strip().split()\n",
    "      data_x[count] = np.asarray(list(map(int, line[1:-1])))\n",
    "      data_y[count] = label2id[line[-1]]\n",
    "      count += 1\n",
    "  data_x = torch.from_numpy(data_x).to(torch.float32)\n",
    "  data_y = torch.from_numpy(data_y).to(torch.int64)\n",
    "  print(data_x.shape)\n",
    "  print(data_y.shape)\n",
    "\n",
    "  # 10% for validation\n",
    "  shuffle = np.random.permutation(data_x.shape[0])\n",
    "  trainid = shuffle[:int(shuffle.shape[0]*0.9)]\n",
    "  testid = shuffle[int(shuffle.shape[0]*0.9):]\n",
    "\n",
    "  test_x = data_x[testid].cuda()\n",
    "  test_y = data_y[testid].cuda()\n",
    "  train_x = data_x[trainid].cuda()\n",
    "  train_y = data_y[trainid].cuda()\n",
    "\n",
    "  # loader\n",
    "  trainset = CoraDataset(train_x, train_y)\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, 8, shuffle=True)\n",
    "  testset = CoraDataset(test_x, test_y)\n",
    "  testloader = torch.utils.data.DataLoader(testset, 8, shuffle=False)\n",
    "\n",
    "  model = train(trainloader, featureSize, 20)\n",
    "  test(trainloader, testloader, model, trainid.shape[0], testid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6D050knh_hAJ"
   },
   "outputs": [],
   "source": [
    "def combinemode(p, q, featureSize):\n",
    "  nx_G = read_graph()\n",
    "  G = Graph(nx_G, False, p=p, q=q)\n",
    "  G.preprocess_transition_probs()\n",
    "  walks = G.simulate_walks(10, 80)\n",
    "  model = learn_embeddings(walks, featureSize)\n",
    "\n",
    "  # prepare training data\n",
    "  # read training nodes\n",
    "  import torch\n",
    "  import torch.nn.functional as F\n",
    "  import numpy as np\n",
    "  label2id = {\n",
    "      'Case_Based' : 0,\n",
    "      'Genetic_Algorithms' : 1,\n",
    "      'Neural_Networks' : 2,\n",
    "      'Probabilistic_Methods' : 3,\n",
    "      'Reinforcement_Learning' : 4,\n",
    "      'Rule_Learning' : 5,\n",
    "      'Theory' : 6,\n",
    "  }\n",
    "  data_x = np.zeros((2708, featureSize+1433))\n",
    "  data_y = np.zeros(2708)\n",
    "  count = 0\n",
    "  with open('./cora/cora.content', 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.strip().split()\n",
    "      data_x[count][:featureSize] = model.wv[line[0]]\n",
    "      data_x[count][featureSize:] = np.asarray(list(map(int, line[1:-1])))\n",
    "      data_y[count] = label2id[line[-1]]\n",
    "      count += 1\n",
    "  data_x = torch.from_numpy(data_x).to(torch.float32)\n",
    "  data_y = torch.from_numpy(data_y).to(torch.int64)\n",
    "  featureSize += 1433\n",
    "  print(data_x.shape)\n",
    "  print(data_y.shape)\n",
    "\n",
    "  # 10% for validation\n",
    "  shuffle = np.random.permutation(data_x.shape[0])\n",
    "  trainid = shuffle[:int(shuffle.shape[0]*0.9)]\n",
    "  testid = shuffle[int(shuffle.shape[0]*0.9):]\n",
    "\n",
    "  test_x = data_x[testid].cuda()\n",
    "  test_y = data_y[testid].cuda()\n",
    "  train_x = data_x[trainid].cuda()\n",
    "  train_y = data_y[trainid].cuda()\n",
    "\n",
    "  # loader\n",
    "  trainset = CoraDataset(train_x, train_y)\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, 8, shuffle=True)\n",
    "  testset = CoraDataset(test_x, test_y)\n",
    "  testloader = torch.utils.data.DataLoader(testset, 8, shuffle=False)\n",
    "\n",
    "  model = train(trainloader, featureSize, 40)\n",
    "  test(trainloader, testloader, model, trainid.shape[0], testid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nd4z_z9-IrU",
    "outputId": "63296d70-9dff-4411-d560-af8777071395"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "torch.Size([2708, 128])\n",
      "torch.Size([2708])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]           2,064\n",
      "              ReLU-2                   [-1, 16]               0\n",
      "            Linear-3                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 2,183\n",
      "Trainable params: 2,183\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "[1,   100] loss: 1.796\n",
      "[1,   200] loss: 1.420\n",
      "[1,   300] loss: 1.134\n",
      "[2,   100] loss: 0.907\n",
      "[2,   200] loss: 0.799\n",
      "[2,   300] loss: 0.730\n",
      "[3,   100] loss: 0.639\n",
      "[3,   200] loss: 0.685\n",
      "[3,   300] loss: 0.674\n",
      "[4,   100] loss: 0.615\n",
      "[4,   200] loss: 0.589\n",
      "[4,   300] loss: 0.622\n",
      "[5,   100] loss: 0.626\n",
      "[5,   200] loss: 0.549\n",
      "[5,   300] loss: 0.560\n",
      "[6,   100] loss: 0.555\n",
      "[6,   200] loss: 0.502\n",
      "[6,   300] loss: 0.596\n",
      "[7,   100] loss: 0.564\n",
      "[7,   200] loss: 0.473\n",
      "[7,   300] loss: 0.579\n",
      "[8,   100] loss: 0.541\n",
      "[8,   200] loss: 0.550\n",
      "[8,   300] loss: 0.495\n",
      "[9,   100] loss: 0.483\n",
      "[9,   200] loss: 0.520\n",
      "[9,   300] loss: 0.559\n",
      "[10,   100] loss: 0.481\n",
      "[10,   200] loss: 0.518\n",
      "[10,   300] loss: 0.512\n",
      "[11,   100] loss: 0.497\n",
      "[11,   200] loss: 0.511\n",
      "[11,   300] loss: 0.485\n",
      "[12,   100] loss: 0.500\n",
      "[12,   200] loss: 0.509\n",
      "[12,   300] loss: 0.452\n",
      "[13,   100] loss: 0.481\n",
      "[13,   200] loss: 0.472\n",
      "[13,   300] loss: 0.500\n",
      "[14,   100] loss: 0.492\n",
      "[14,   200] loss: 0.459\n",
      "[14,   300] loss: 0.469\n",
      "[15,   100] loss: 0.456\n",
      "[15,   200] loss: 0.479\n",
      "[15,   300] loss: 0.488\n",
      "[16,   100] loss: 0.473\n",
      "[16,   200] loss: 0.436\n",
      "[16,   300] loss: 0.492\n",
      "[17,   100] loss: 0.484\n",
      "[17,   200] loss: 0.444\n",
      "[17,   300] loss: 0.450\n",
      "[18,   100] loss: 0.462\n",
      "[18,   200] loss: 0.424\n",
      "[18,   300] loss: 0.477\n",
      "[19,   100] loss: 0.479\n",
      "[19,   200] loss: 0.427\n",
      "[19,   300] loss: 0.436\n",
      "[20,   100] loss: 0.421\n",
      "[20,   200] loss: 0.409\n",
      "[20,   300] loss: 0.488\n",
      "[21,   100] loss: 0.431\n",
      "[21,   200] loss: 0.436\n",
      "[21,   300] loss: 0.445\n",
      "[22,   100] loss: 0.420\n",
      "[22,   200] loss: 0.436\n",
      "[22,   300] loss: 0.453\n",
      "[23,   100] loss: 0.435\n",
      "[23,   200] loss: 0.391\n",
      "[23,   300] loss: 0.475\n",
      "[24,   100] loss: 0.395\n",
      "[24,   200] loss: 0.416\n",
      "[24,   300] loss: 0.476\n",
      "[25,   100] loss: 0.395\n",
      "[25,   200] loss: 0.485\n",
      "[25,   300] loss: 0.397\n",
      "[26,   100] loss: 0.448\n",
      "[26,   200] loss: 0.370\n",
      "[26,   300] loss: 0.454\n",
      "[27,   100] loss: 0.385\n",
      "[27,   200] loss: 0.436\n",
      "[27,   300] loss: 0.425\n",
      "[28,   100] loss: 0.407\n",
      "[28,   200] loss: 0.403\n",
      "[28,   300] loss: 0.436\n",
      "[29,   100] loss: 0.416\n",
      "[29,   200] loss: 0.419\n",
      "[29,   300] loss: 0.413\n",
      "[30,   100] loss: 0.412\n",
      "[30,   200] loss: 0.399\n",
      "[30,   300] loss: 0.402\n",
      "[31,   100] loss: 0.411\n",
      "[31,   200] loss: 0.397\n",
      "[31,   300] loss: 0.413\n",
      "[32,   100] loss: 0.397\n",
      "[32,   200] loss: 0.418\n",
      "[32,   300] loss: 0.401\n",
      "[33,   100] loss: 0.369\n",
      "[33,   200] loss: 0.418\n",
      "[33,   300] loss: 0.426\n",
      "[34,   100] loss: 0.390\n",
      "[34,   200] loss: 0.422\n",
      "[34,   300] loss: 0.394\n",
      "[35,   100] loss: 0.354\n",
      "[35,   200] loss: 0.400\n",
      "[35,   300] loss: 0.429\n",
      "[36,   100] loss: 0.419\n",
      "[36,   200] loss: 0.424\n",
      "[36,   300] loss: 0.343\n",
      "[37,   100] loss: 0.365\n",
      "[37,   200] loss: 0.412\n",
      "[37,   300] loss: 0.401\n",
      "[38,   100] loss: 0.416\n",
      "[38,   200] loss: 0.422\n",
      "[38,   300] loss: 0.327\n",
      "[39,   100] loss: 0.366\n",
      "[39,   200] loss: 0.387\n",
      "[39,   300] loss: 0.406\n",
      "[40,   100] loss: 0.329\n",
      "[40,   200] loss: 0.413\n",
      "[40,   300] loss: 0.406\n",
      "Accuracy of the network on the 2437 train data: 88 %\n",
      "Accuracy of the network on the 271 test data: 87 %\n"
     ]
    }
   ],
   "source": [
    "node2vecmode(p=1., q=1., featureSize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7yh5lun_ble",
    "outputId": "e09f7913-7139-4e94-89a2-1033db2a4f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]          22,944\n",
      "              ReLU-2                   [-1, 16]               0\n",
      "            Linear-3                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 23,063\n",
      "Trainable params: 23,063\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n",
      "[1,   100] loss: 1.898\n",
      "[1,   200] loss: 1.638\n",
      "[1,   300] loss: 1.331\n",
      "[2,   100] loss: 0.970\n",
      "[2,   200] loss: 0.819\n",
      "[2,   300] loss: 0.804\n",
      "[3,   100] loss: 0.544\n",
      "[3,   200] loss: 0.538\n",
      "[3,   300] loss: 0.530\n",
      "[4,   100] loss: 0.379\n",
      "[4,   200] loss: 0.367\n",
      "[4,   300] loss: 0.381\n",
      "[5,   100] loss: 0.277\n",
      "[5,   200] loss: 0.270\n",
      "[5,   300] loss: 0.269\n",
      "[6,   100] loss: 0.201\n",
      "[6,   200] loss: 0.208\n",
      "[6,   300] loss: 0.200\n",
      "[7,   100] loss: 0.146\n",
      "[7,   200] loss: 0.160\n",
      "[7,   300] loss: 0.157\n",
      "[8,   100] loss: 0.134\n",
      "[8,   200] loss: 0.112\n",
      "[8,   300] loss: 0.112\n",
      "[9,   100] loss: 0.081\n",
      "[9,   200] loss: 0.099\n",
      "[9,   300] loss: 0.099\n",
      "[10,   100] loss: 0.074\n",
      "[10,   200] loss: 0.082\n",
      "[10,   300] loss: 0.065\n",
      "[11,   100] loss: 0.053\n",
      "[11,   200] loss: 0.059\n",
      "[11,   300] loss: 0.067\n",
      "[12,   100] loss: 0.051\n",
      "[12,   200] loss: 0.045\n",
      "[12,   300] loss: 0.048\n",
      "[13,   100] loss: 0.039\n",
      "[13,   200] loss: 0.037\n",
      "[13,   300] loss: 0.040\n",
      "[14,   100] loss: 0.037\n",
      "[14,   200] loss: 0.038\n",
      "[14,   300] loss: 0.023\n",
      "[15,   100] loss: 0.029\n",
      "[15,   200] loss: 0.030\n",
      "[15,   300] loss: 0.024\n",
      "[16,   100] loss: 0.022\n",
      "[16,   200] loss: 0.022\n",
      "[16,   300] loss: 0.024\n",
      "[17,   100] loss: 0.016\n",
      "[17,   200] loss: 0.023\n",
      "[17,   300] loss: 0.019\n",
      "[18,   100] loss: 0.015\n",
      "[18,   200] loss: 0.020\n",
      "[18,   300] loss: 0.014\n",
      "[19,   100] loss: 0.014\n",
      "[19,   200] loss: 0.014\n",
      "[19,   300] loss: 0.013\n",
      "[20,   100] loss: 0.012\n",
      "[20,   200] loss: 0.009\n",
      "[20,   300] loss: 0.013\n",
      "Accuracy of the network on the 2437 train data: 99 %\n",
      "Accuracy of the network on the 271 test data: 74 %\n"
     ]
    }
   ],
   "source": [
    "textfeaturemode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cdsPJ1E_fLK",
    "outputId": "60908129-3f2e-4a02-cc7f-929c94581e24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "torch.Size([2708, 1561])\n",
      "torch.Size([2708])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]          24,992\n",
      "              ReLU-2                   [-1, 16]               0\n",
      "            Linear-3                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 25,111\n",
      "Trainable params: 25,111\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.10\n",
      "----------------------------------------------------------------\n",
      "[1,   100] loss: 1.741\n",
      "[1,   200] loss: 1.310\n",
      "[1,   300] loss: 0.872\n",
      "[2,   100] loss: 0.603\n",
      "[2,   200] loss: 0.542\n",
      "[2,   300] loss: 0.484\n",
      "[3,   100] loss: 0.392\n",
      "[3,   200] loss: 0.322\n",
      "[3,   300] loss: 0.321\n",
      "[4,   100] loss: 0.230\n",
      "[4,   200] loss: 0.229\n",
      "[4,   300] loss: 0.285\n",
      "[5,   100] loss: 0.154\n",
      "[5,   200] loss: 0.177\n",
      "[5,   300] loss: 0.212\n",
      "[6,   100] loss: 0.155\n",
      "[6,   200] loss: 0.127\n",
      "[6,   300] loss: 0.130\n",
      "[7,   100] loss: 0.092\n",
      "[7,   200] loss: 0.104\n",
      "[7,   300] loss: 0.115\n",
      "[8,   100] loss: 0.078\n",
      "[8,   200] loss: 0.077\n",
      "[8,   300] loss: 0.086\n",
      "[9,   100] loss: 0.066\n",
      "[9,   200] loss: 0.056\n",
      "[9,   300] loss: 0.064\n",
      "[10,   100] loss: 0.043\n",
      "[10,   200] loss: 0.059\n",
      "[10,   300] loss: 0.047\n",
      "[11,   100] loss: 0.033\n",
      "[11,   200] loss: 0.044\n",
      "[11,   300] loss: 0.042\n",
      "[12,   100] loss: 0.027\n",
      "[12,   200] loss: 0.034\n",
      "[12,   300] loss: 0.033\n",
      "[13,   100] loss: 0.023\n",
      "[13,   200] loss: 0.022\n",
      "[13,   300] loss: 0.030\n",
      "[14,   100] loss: 0.019\n",
      "[14,   200] loss: 0.019\n",
      "[14,   300] loss: 0.022\n",
      "[15,   100] loss: 0.015\n",
      "[15,   200] loss: 0.017\n",
      "[15,   300] loss: 0.016\n",
      "[16,   100] loss: 0.011\n",
      "[16,   200] loss: 0.013\n",
      "[16,   300] loss: 0.015\n",
      "[17,   100] loss: 0.011\n",
      "[17,   200] loss: 0.009\n",
      "[17,   300] loss: 0.011\n",
      "[18,   100] loss: 0.008\n",
      "[18,   200] loss: 0.009\n",
      "[18,   300] loss: 0.008\n",
      "[19,   100] loss: 0.008\n",
      "[19,   200] loss: 0.006\n",
      "[19,   300] loss: 0.007\n",
      "[20,   100] loss: 0.005\n",
      "[20,   200] loss: 0.006\n",
      "[20,   300] loss: 0.006\n",
      "[21,   100] loss: 0.004\n",
      "[21,   200] loss: 0.004\n",
      "[21,   300] loss: 0.006\n",
      "[22,   100] loss: 0.004\n",
      "[22,   200] loss: 0.004\n",
      "[22,   300] loss: 0.004\n",
      "[23,   100] loss: 0.003\n",
      "[23,   200] loss: 0.003\n",
      "[23,   300] loss: 0.004\n",
      "[24,   100] loss: 0.003\n",
      "[24,   200] loss: 0.003\n",
      "[24,   300] loss: 0.003\n",
      "[25,   100] loss: 0.002\n",
      "[25,   200] loss: 0.002\n",
      "[25,   300] loss: 0.002\n",
      "[26,   100] loss: 0.002\n",
      "[26,   200] loss: 0.002\n",
      "[26,   300] loss: 0.002\n",
      "[27,   100] loss: 0.002\n",
      "[27,   200] loss: 0.002\n",
      "[27,   300] loss: 0.002\n",
      "[28,   100] loss: 0.001\n",
      "[28,   200] loss: 0.001\n",
      "[28,   300] loss: 0.002\n",
      "[29,   100] loss: 0.001\n",
      "[29,   200] loss: 0.001\n",
      "[29,   300] loss: 0.001\n",
      "[30,   100] loss: 0.001\n",
      "[30,   200] loss: 0.001\n",
      "[30,   300] loss: 0.001\n",
      "[31,   100] loss: 0.001\n",
      "[31,   200] loss: 0.001\n",
      "[31,   300] loss: 0.001\n",
      "[32,   100] loss: 0.001\n",
      "[32,   200] loss: 0.001\n",
      "[32,   300] loss: 0.001\n",
      "[33,   100] loss: 0.001\n",
      "[33,   200] loss: 0.001\n",
      "[33,   300] loss: 0.001\n",
      "[34,   100] loss: 0.000\n",
      "[34,   200] loss: 0.000\n",
      "[34,   300] loss: 0.001\n",
      "[35,   100] loss: 0.000\n",
      "[35,   200] loss: 0.000\n",
      "[35,   300] loss: 0.001\n",
      "[36,   100] loss: 0.000\n",
      "[36,   200] loss: 0.000\n",
      "[36,   300] loss: 0.000\n",
      "[37,   100] loss: 0.000\n",
      "[37,   200] loss: 0.000\n",
      "[37,   300] loss: 0.000\n",
      "[38,   100] loss: 0.000\n",
      "[38,   200] loss: 0.000\n",
      "[38,   300] loss: 0.000\n",
      "[39,   100] loss: 0.000\n",
      "[39,   200] loss: 0.000\n",
      "[39,   300] loss: 0.000\n",
      "[40,   100] loss: 0.000\n",
      "[40,   200] loss: 0.000\n",
      "[40,   300] loss: 0.000\n",
      "Accuracy of the network on the 2437 train data: 100 %\n",
      "Accuracy of the network on the 271 test data: 85 %\n"
     ]
    }
   ],
   "source": [
    "combinemode(p=1., q=1., featureSize=128)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "q3_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
