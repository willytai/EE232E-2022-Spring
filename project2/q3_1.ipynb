{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxgq89zIfzP-",
        "outputId": "4c04ca6f-2d3f-41c1-a0ca-7725832991e7"
      },
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/dgl-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 800 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ref: https://cnvrg.io/graph-neural-networks/"
      ],
      "metadata": {
        "id": "EFsGDWWrIAGT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "721v55ACgKRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edc9ec2-f58e-4c1a-994a-64c3a07e6e06"
      },
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJyIOBihbOBB",
        "outputId": "f77fb97e-7e3b-4dac-d24b-ef3fe8671538"
      },
      "source": [
        "import dgl.data\n",
        "\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "print('Number of categories:', dataset.num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "Number of categories: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = dataset[0]\n",
        "# num_class = g.num_classes\n",
        "# get node feature\n",
        "feat = g.ndata['feat']\n",
        "# get data split\n",
        "train_mask = g.ndata['train_mask']\n",
        "val_mask = g.ndata['val_mask']\n",
        "test_mask = g.ndata['test_mask']\n",
        "# get labels\n",
        "label = g.ndata['label']\n",
        "print(feat.shape, train_mask.shape, val_mask.shape, test_mask.shape, label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOufPKbrAPXk",
        "outputId": "be088717-f1e6-40c7-bf0a-4d48617df2f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 1433]) torch.Size([2708]) torch.Size([2708]) torch.Size([2708]) torch.Size([2708])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g.ndata['feat'].shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C1IzVC-CETT",
        "outputId": "97281a45-d95b-4183-a23b-fe4488cf6757"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1433"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnsD-1W9gUwp"
      },
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, temp_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "\n",
        "        return h\n",
        "\n",
        "# Create the model with given dimensions\n",
        "# model = GCN(g.ndata['feat'].shape[1], 160, dataset.num_classes)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbecjHODgapM",
        "outputId": "e6d39137-ed31-4e9e-a8d5-95af586ebb5f"
      },
      "source": [
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata['feat']\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    for e in range(1000):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
        "model = GCN(g.ndata['feat'].shape[1], 160, 160, dataset.num_classes)\n",
        "train(g, model)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.946, val acc: 0.114 (best 0.114), test acc: 0.091 (best 0.091)\n",
            "In epoch 5, loss: 1.844, val acc: 0.698 (best 0.698), test acc: 0.724 (best 0.724)\n",
            "In epoch 10, loss: 1.683, val acc: 0.720 (best 0.722), test acc: 0.756 (best 0.764)\n",
            "In epoch 15, loss: 1.469, val acc: 0.738 (best 0.738), test acc: 0.754 (best 0.754)\n",
            "In epoch 20, loss: 1.215, val acc: 0.766 (best 0.766), test acc: 0.782 (best 0.782)\n",
            "In epoch 25, loss: 0.946, val acc: 0.778 (best 0.778), test acc: 0.784 (best 0.787)\n",
            "In epoch 30, loss: 0.696, val acc: 0.782 (best 0.782), test acc: 0.792 (best 0.792)\n",
            "In epoch 35, loss: 0.492, val acc: 0.786 (best 0.786), test acc: 0.792 (best 0.793)\n",
            "In epoch 40, loss: 0.342, val acc: 0.786 (best 0.786), test acc: 0.797 (best 0.793)\n",
            "In epoch 45, loss: 0.239, val acc: 0.782 (best 0.788), test acc: 0.806 (best 0.802)\n",
            "In epoch 50, loss: 0.169, val acc: 0.784 (best 0.788), test acc: 0.802 (best 0.802)\n",
            "In epoch 55, loss: 0.122, val acc: 0.788 (best 0.788), test acc: 0.799 (best 0.802)\n",
            "In epoch 60, loss: 0.091, val acc: 0.788 (best 0.790), test acc: 0.795 (best 0.797)\n",
            "In epoch 65, loss: 0.069, val acc: 0.784 (best 0.790), test acc: 0.791 (best 0.797)\n",
            "In epoch 70, loss: 0.054, val acc: 0.782 (best 0.790), test acc: 0.787 (best 0.797)\n",
            "In epoch 75, loss: 0.044, val acc: 0.782 (best 0.790), test acc: 0.787 (best 0.797)\n",
            "In epoch 80, loss: 0.036, val acc: 0.784 (best 0.790), test acc: 0.784 (best 0.797)\n",
            "In epoch 85, loss: 0.031, val acc: 0.782 (best 0.790), test acc: 0.778 (best 0.797)\n",
            "In epoch 90, loss: 0.026, val acc: 0.784 (best 0.790), test acc: 0.777 (best 0.797)\n",
            "In epoch 95, loss: 0.023, val acc: 0.784 (best 0.790), test acc: 0.775 (best 0.797)\n",
            "In epoch 100, loss: 0.020, val acc: 0.786 (best 0.790), test acc: 0.777 (best 0.797)\n",
            "In epoch 105, loss: 0.018, val acc: 0.788 (best 0.790), test acc: 0.777 (best 0.797)\n",
            "In epoch 110, loss: 0.016, val acc: 0.788 (best 0.790), test acc: 0.777 (best 0.797)\n",
            "In epoch 115, loss: 0.015, val acc: 0.788 (best 0.790), test acc: 0.776 (best 0.797)\n",
            "In epoch 120, loss: 0.014, val acc: 0.788 (best 0.790), test acc: 0.775 (best 0.797)\n",
            "In epoch 125, loss: 0.013, val acc: 0.788 (best 0.790), test acc: 0.773 (best 0.797)\n",
            "In epoch 130, loss: 0.012, val acc: 0.788 (best 0.790), test acc: 0.773 (best 0.797)\n",
            "In epoch 135, loss: 0.011, val acc: 0.786 (best 0.790), test acc: 0.774 (best 0.797)\n",
            "In epoch 140, loss: 0.010, val acc: 0.786 (best 0.790), test acc: 0.773 (best 0.797)\n",
            "In epoch 145, loss: 0.009, val acc: 0.786 (best 0.790), test acc: 0.774 (best 0.797)\n",
            "In epoch 150, loss: 0.009, val acc: 0.786 (best 0.790), test acc: 0.774 (best 0.797)\n",
            "In epoch 155, loss: 0.008, val acc: 0.784 (best 0.790), test acc: 0.773 (best 0.797)\n",
            "In epoch 160, loss: 0.008, val acc: 0.784 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 165, loss: 0.007, val acc: 0.784 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 170, loss: 0.007, val acc: 0.784 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 175, loss: 0.007, val acc: 0.784 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 180, loss: 0.006, val acc: 0.784 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 185, loss: 0.006, val acc: 0.780 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 190, loss: 0.006, val acc: 0.780 (best 0.790), test acc: 0.772 (best 0.797)\n",
            "In epoch 195, loss: 0.005, val acc: 0.780 (best 0.790), test acc: 0.771 (best 0.797)\n",
            "In epoch 200, loss: 0.005, val acc: 0.780 (best 0.790), test acc: 0.771 (best 0.797)\n",
            "In epoch 205, loss: 0.005, val acc: 0.778 (best 0.790), test acc: 0.770 (best 0.797)\n",
            "In epoch 210, loss: 0.005, val acc: 0.778 (best 0.790), test acc: 0.770 (best 0.797)\n",
            "In epoch 215, loss: 0.005, val acc: 0.780 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 220, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 225, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 230, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 235, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 240, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 245, loss: 0.004, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 250, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 255, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 260, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 265, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 270, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 275, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 280, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 285, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.769 (best 0.797)\n",
            "In epoch 290, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 295, loss: 0.003, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 300, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 305, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 310, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 315, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.768 (best 0.797)\n",
            "In epoch 320, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 325, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 330, loss: 0.002, val acc: 0.778 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 335, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 340, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 345, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 350, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 355, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 360, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 365, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 370, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 375, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 380, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 385, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 390, loss: 0.002, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 395, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 400, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 405, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 410, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 415, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 420, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 425, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 430, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.767 (best 0.797)\n",
            "In epoch 435, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 440, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 445, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 450, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 455, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 460, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 465, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 470, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 475, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.766 (best 0.797)\n",
            "In epoch 480, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 485, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 490, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 495, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 500, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 505, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 510, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 515, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 520, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 525, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 530, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 535, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 540, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 545, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 550, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 555, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 560, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 565, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 570, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 575, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 580, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 585, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 590, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 595, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 600, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 605, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 610, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 615, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 620, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 625, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 630, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 635, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.765 (best 0.797)\n",
            "In epoch 640, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 645, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 650, loss: 0.001, val acc: 0.776 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 655, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 660, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 665, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 670, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 675, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 680, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 685, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 690, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 695, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 700, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 705, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 710, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 715, loss: 0.001, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 720, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 725, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 730, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 735, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 740, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 745, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 750, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 755, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 760, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 765, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 770, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 775, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 780, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 785, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 790, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 795, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 800, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 805, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 810, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 815, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 820, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 825, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 830, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 835, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 840, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 845, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 850, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 855, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 860, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 865, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 870, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 875, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 880, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 885, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 890, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 895, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 900, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.764 (best 0.797)\n",
            "In epoch 905, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 910, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 915, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 920, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 925, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 930, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 935, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 940, loss: 0.000, val acc: 0.778 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 945, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 950, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 955, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 960, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 965, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 970, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 975, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 980, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 985, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 990, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n",
            "In epoch 995, loss: 0.000, val acc: 0.776 (best 0.790), test acc: 0.763 (best 0.797)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNNbgRaXh1f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0LMFawr3H_Hc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}